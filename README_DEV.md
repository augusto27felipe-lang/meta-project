# README_DEV ‚Äî Projeto Scrapers V3.1

## üéØ Objetivo
Estamos construindo uma **su√≠te de scrapers** dividida em 3 m√≥dulos independentes:
1. **Scraper de Keywords**
2. **Scraper de An√∫ncios**
3. **Scraper de Dom√≠nios**

Cada scraper tem tr√™s etapas:
- **Configura√ß√µes**: navegador headless, credenciais, planilha, par√¢metros espec√≠ficos.
- **Input**: carregar TXT/CSV ou dados da etapa anterior (keywords, an√∫ncios, dom√≠nios).
- **Execu√ß√£o**: rodar o scraping, gerar CSV, log e metadados (`run_id`).

O **Monitor** √© compartilhado entre todos os scrapers, exibindo runs com metadados e preview de CSV/log.

---

## üèó Arquitetura

### Backend (FastAPI)
- `routes_keywords.py` ‚Üí endpoints `/keywords/*`
- `routes_ads.py` ‚Üí endpoints `/ads/*`
- `routes_domains.py` ‚Üí endpoints `/domains/*`
- `routes_monitor.py` ‚Üí endpoints `/monitor/*` (compartilhado)

Cada `/run` gera:
- `runs/<run_id>.json` (metadados: tipo, start, end, dura√ß√£o, count)
- `runs/<run_id>.done` (sentinela de conclus√£o)
- `data/<type>-<run_id>.csv`
- `logs/<type>-<run_id>.log`

### Frontend (GUI Tkinter)
- `gui.py` ‚Üí GUI principal com abas
- `gui_keywords.py` ‚Üí aba Keywords
- `gui_ads.py` ‚Üí aba An√∫ncios
- `gui_domains.py` ‚Üí aba Dom√≠nios
- Monitor tab ‚Üí Treeview com colunas (`run_id`, `type`, `completed_at`, `duration`, `count`), preview CSV/log, ordena√ß√£o, limpar hist√≥rico, testar conex√£o.

---

## üîß Recursos j√° implementados
- Execu√ß√£o com `run_id` e artefatos por run.
- Monitor com preview de CSV/log, ordena√ß√£o por colunas (‚ñ≤/‚ñº), limpar hist√≥rico, testar conex√£o.
- GUI modular com abas.
- Testes automatizados (`test_monitor_endpoints.py`).

---

## üöÄ Pr√≥ximos passos
- [ ] Conectar bot√µes de execu√ß√£o de cada aba aos endpoints correspondentes.
- [ ] Garantir que o Monitor mostre runs de todos os scrapers (campo `type`).
- [ ] Unificar a aba de Configura√ß√µes no futuro (uma s√≥ config global).
- [ ] Melhorar UX do preview CSV (modal interno em vez de abrir Explorer).
- [ ] Adicionar testes unit√°rios para cada scraper.

---

## üí° Instru√ß√µes para Copilot
- Sempre sugerir c√≥digo modularizado por scraper (keywords, ads, domains).
- Reaproveitar fun√ß√µes utilit√°rias j√° existentes (save_csv, logs, run_id).
- Usar `type` nos metadados para diferenciar runs no Monitor.
- GUI deve manter padr√£o: Config ‚Üí Input ‚Üí Execu√ß√£o em cada aba.
- Evitar duplica√ß√£o de l√≥gica: se algo √© comum, mover para `utils/`.

---

## üñ•Ô∏è Setup r√°pido (Windows PowerShell)

Passos m√≠nimos para configurar o ambiente de desenvolvimento no Windows e rodar testes.

1) Criar e ativar virtualenv:

```powershell
cd "C:\Users\Felipe Augusto\Documents\Projeto V3.1"
python -m venv .venv
# Ativar (padr√£o PowerShell):
.\.venv\Scripts\Activate.ps1
# Se PowerShell bloquear execu√ß√£o de scripts temporariamente na sess√£o atual:
# Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
# Em alternativa, voc√™ pode usar o execut√°vel do venv sem ativar:
# .\.venv\Scripts\python.exe -m pip install -r requirements.txt
```

2) Instalar depend√™ncias:

```powershell
.\.venv\Scripts\python.exe -m pip install -r requirements.txt
```

3) (Opcional) Instalar navegadores para Playwright (se for rodar testes E2E que abrem browser):

```powershell
.\.venv\Scripts\python.exe -m playwright install
```

4) Rodar testes:

```powershell
# Rodar su√≠te completa
.\.venv\Scripts\python.exe -m pytest -q

# Rodar testes espec√≠ficos
.\.venv\Scripts\python.exe -m pytest tests/test_api_integration.py -q
```

5) Rodar o backend/gui:

```powershell
# Usar launcher (GUI com bot√µes) ou rodar uvicorn diretamente
python launcher.py
# ou
.\.venv\Scripts\python.exe -m uvicorn backend.etapa4_service.main:app --reload
```

### Notas r√°pidas
- Se `Activate.ps1` falhar por causa da ExecutionPolicy, use `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass` na sess√£o atual.
- Adicionei `httpx` em `requirements.txt` para corrigir falha no `TestClient` dos testes.
- Resultado da su√≠te local: `13 passed`.

---

## Teste r√°pido: modo CLI do `launcher.py`

O `launcher.py` agora suporta duas flags de linha de comando √∫teis para testes r√°pidos do ciclo start‚Üístop do backend:

- `--auto-start` ‚Äî inicia o backend automaticamente ao abrir o launcher.
- `--auto-stop-seconds=N` ‚Äî quando usado junto com `--auto-start`, agenda um `stop` ap√≥s N segundos.

Exemplo (PowerShell):

```powershell
# Inicia o launcher, que por sua vez inicia o backend; o backend ser√° parado ap√≥s 5 segundos
.\.venv\Scripts\python.exe launcher.py --auto-start --auto-stop-seconds=5
```

Comportamento e observa√ß√µes:
- O servidor escreve um PID file em `logs/uvicorn.pid` ao iniciar; o `launcher` usa esse arquivo para identificar o processo quando pede `stop`.
- Se o processo do servidor for terminado externamente, o `launcher` tentar√° limpar o PID stale na pr√≥xima inicializa√ß√£o.
- A checagem de exist√™ncia de PIDs usa `psutil` se dispon√≠vel, mas o c√≥digo est√° protegido por `try/except` ‚Äî `psutil` √© opcional.

Nota sobre `psutil`:

- Recomendamos instalar `psutil` em ambientes de desenvolvimento/CI para checagens de PID mais confi√°veis. Voc√™ pode instal√°-lo com:

```powershell
.\.venv\Scripts\python.exe -m pip install -r requirements-dev.txt
```

O projeto funciona sem `psutil`, mas o launcher far√° verifica√ß√µes mais robustas quando ele estiver dispon√≠vel.

Use este modo para um smoke test r√°pido: iniciar o launcher em modo auto, checar `http://127.0.0.1:8000/health`, e confirmar que o PID file √© removido quando o launcher para o backend.

---

## üî¨ Smoke test integrado (start ‚Üí /health ‚Üí stop)

Use este procedimento para validar o ciclo completo de start‚Üíhealth‚Üístop usando o modo CLI do `launcher.py`.

Passos (PowerShell, no diret√≥rio do projeto):

```powershell
# 1) Inicie o launcher em modo auto (backend ser√° parado automaticamente ap√≥s 8 segundos)
.\.venv\Scripts\python.exe launcher.py --auto-start --auto-stop-seconds=8

# 2) Em outra sess√£o (ou logo ap√≥s iniciar), verifique o endpoint de health
Start-Sleep -s 2
Invoke-WebRequest -UseBasicParsing -Uri http://127.0.0.1:8000/health -TimeoutSec 5 | Select-Object -ExpandProperty Content

# 3) Cheque que o PID file foi criado imediatamente ap√≥s o start
Get-Content .\logs\uvicorn.pid

# 4) Aguarde o auto-stop completar (8s + margem) e confirme que o PID file foi removido
Start-Sleep -s 10
if (Test-Path .\logs\uvicorn.pid) { Write-Output "PID_FILE_EXISTS" } else { Write-Output "PID_FILE_REMOVED" }
```

O resultado esperado:
- A chamada a `/health` retorna um payload JSON (ou texto simples) indicando que o backend est√° saud√°vel.
- `logs/uvicorn.pid` √© criado logo ap√≥s o start e cont√©m o PID do processo do uvicorn.
- Ap√≥s o auto-stop, `logs/uvicorn.pid` n√£o existe mais.

Notas:
- Se a checagem de `/health` falhar, cole o conte√∫do de `logs/server.err` e `logs/server.log` para diagn√≥stico.
- Em CI recomendamos aumentar `--auto-stop-seconds` se o ambiente do runner for mais lento.

---

## üó∫Ô∏è Roadmap / Checklist (curto prazo)

- [x] Dockerfile m√≠nimo criado
- [ ] CI: workflow para buildar imagem e rodar testes dentro do container
- [ ] Deploy b√°sico: escolher destino (Heroku / Railway / VM) e criar pipeline
- [ ] Integra√ß√£o Codecov: adicionar step no CI e configurar `CODECOV_TOKEN` (secret)
- [ ] Documentar contrato de erro (`runs failed`) ‚Äî exemplos JSON
- [ ] Validar imagem localmente (requer Docker local)

---

## ‚öôÔ∏è Usando `UVICORN_PORT` / `--port` para testes locais

Para facilitar smoke tests locais em portas alternativas (evitando conflitos), `scripts/start_uvicorn_bg.py` aceita:

- vari√°vel de ambiente `UVICORN_PORT` (ex.: `8001`), ou
- argumento CLI `--port=8001`.

Exemplos (PowerShell):

```powershell
# Usando CLI
.\.venv\Scripts\python.exe scripts\start_uvicorn_bg.py --port=8001

# Usando vari√°vel de ambiente
$env:UVICORN_PORT = '8002'
.\.venv\Scripts\python.exe scripts\start_uvicorn_bg.py
```

Isso facilita rodar m√∫ltiplas inst√¢ncias em paralelo para troubleshooting.

## üîê Codecov (opcional)

Se voc√™ quiser habilitar upload de cobertura no CI, adicione o secret `CODECOV_TOKEN` em Settings ‚Üí Secrets ‚Üí Actions. Com o token presente, o workflow far√° upload de cobertura ao final do build.


## üìú Contrato de erro (resumo)

O formato de erros de execu√ß√£o (quando uma run falha) segue o esqueleto JSON abaixo. Adicione exemplos reais dos logs/artefatos ao documento principal quando houver runs falhas para ajudar debugging.

Exemplo m√≠nimo:

```json
{
	"run_id": "20250929-abcdef",
	"type": "keywords|ads|domains",
	"status": "failed",
	"started_at": "2025-09-29T12:00:00Z",
	"ended_at": "2025-09-29T12:05:05Z",
	"error": {
		"message": "Timeout while fetching search results",
		"step": "fetch_results",
		"stack": "...stack trace or summarized error..."
	},
	"artifacts": {
		"log": "logs/keywords-20250929-abcdef.log",
		"snapshot": "logs/html_failures/20250929-abcdef.html"
	}
}
```

Manter esse padr√£o facilita triagem autom√°tica, agrupamento por erro e upload de pacotes de diagn√≥stico.
